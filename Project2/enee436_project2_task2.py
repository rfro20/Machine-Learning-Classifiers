# -*- coding: utf-8 -*-
"""ENEE436_project2_task2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cHRlf7iXq4ZHQpoLvpFe0kNqjrFrF5BP

# **Building a Convolutional Neural Network with TensorFlow.**
 
 

> Foundations of Machine Learning (ENEE436/ENTS669D) - Spring 2022

> Prof. Behtash Babadi, ECE, UMD

Author: Christos Mavridis (<mavridis@umd.edu>)

## MNIST Dataset Overview

This example is using MNIST handwritten digits. The dataset contains 60,000 examples for training and 10,000 examples for testing. The digits have been size-normalized and centered in a fixed-size image (28x28 pixels) with values from 0 to 1. 

<a title="By Josef Steppan [CC BY-SA 4.0 (https://creativecommons.org/licenses/by-sa/4.0)], from Wikimedia Commons" href="https://commons.wikimedia.org/wiki/File:MnistExamples.png"><img width="512" alt="MnistExamples" src="https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png"/></a>

More info: http://yann.lecun.com/exdb/mnist/

## Load MNIST Dataset
"""

import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
import random
import os
import distutils
if distutils.version.LooseVersion(tf.__version__) <= '2.0':
    raise Exception('This notebook is compatible with TensorFlow 1.14 or higher, for TensorFlow 1.13 or lower please use the previous version at https://github.com/tensorflow/tpu/blob/r1.13/tools/colab/fashion_mnist.ipynb')

(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0
y_train0 = y_train
y_test0= y_test

# add empty color dimension
x_train = np.expand_dims(x_train, -1)
x_test = np.expand_dims(x_test, -1)

print(f'Input image dimension: {x_train.shape[1:]}')

"""# Why CNN?

### CNN Overview

![CNN](http://personal.ie.cuhk.edu.hk/~ccloy/project_target_code/images/fig3.png)

### Convolutions

- Reduce number of weights (weight sharing, Toeplitz matrix)
- Learn linear time-invariant systems (convolutional filters) which were typically used ad-hoc as pre-processing steps and required domain knowledge (e.g. Laplacian filters = edge detectors in images)
- Output size (1D): n-m+1 (n:input size, m:filter size)

<img width="512" alt="MnistExamples" src="https://miro.medium.com/max/2880/0*QS1ArBEUJjjySXhE.png"/>

### Pooling

- Dimensionality reduction
- Build hierarchy 

<img width="512" alt="MnistExamples" src="https://computersciencewiki.org/images/8/8a/MaxpoolSample2.png"/>


### Activation functions

<img width="512" alt="MnistExamples" src="https://miro.medium.com/max/2800/0*44z992IXd9rqyIWk.png"/>

### Softmax --> Gibbs (Boltzmann) distr. --> Probabilities

<img width="512" alt="MnistExamples" src="https://www.andreaperlato.com/img/softmaxfunction.png"/>




### Automatic feature extraction

<img width="512" alt="MnistExamples" src="https://miro.medium.com/max/616/1*Uhr-4VDJD0-gnteUNFzZTw.jpeg"/>

## Building a CNN: Example of LeNet5

![CNN](https://cdn-images-1.medium.com/max/800/0*V1vb9SDnsU1eZQUy.jpg)

#### MNIST Input
    32x32x1 pixels image

#### Architecture
* **Convolutional #1** 
    * Filters: 6
    * Filter size: 5x5 
    * --> Output 28x28x6
    * Activation: `relu`

* **Pooling #1** 
    * The output shape should be 14x14x6.

* **Convolutional #2** 

* **Pooling #2** 

* **Fully Connected #1** outputs 120
    
* **Fully Connected #2** outputs 84
    
* **Fully Connected #3** output 10 (# classes)

## Custom CNN Design

#### MNIST Input
    28x28x1 pixels image
"""

def myCNN():
  
  model = tf.keras.models.Sequential()

  # First Layer
  model.add(tf.keras.layers.Conv2D(filters=6, kernel_size=(5, 5), activation='tanh',input_shape=x_train.shape[1:]))
  model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2,2)))

  # Second Layer
  model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=(3, 3), activation='tanh'))
  model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2,2)))
  
  model.add(tf.keras.layers.Flatten())

  # Fully-connected NNs
  model.add(tf.keras.layers.Dense(units=120, activation='tanh'))
  model.add(tf.keras.layers.Dense(units=84, activation='tanh'))
  
  model.add(tf.keras.layers.Dense(units=10, activation = 'softmax'))
  
  return model

print('Model Structure & Parameters:')
model = myCNN()
#model.summary()

"""# Optimization Parameters

- Stochastic Gradient Descent

<img width="512" alt="MnistExamples" src="https://miro.medium.com/max/425/1*m1KQOLl-qB0mgRq_IWivnQ.png"/>

- Adaptive momentum method (Adam)

<img width="256" alt="MnistExamples" src="https://miro.medium.com/max/380/1*Ti-cvetTBXnTsM6rHUhmlg.png"/>

- Race to global minima

<img width="512" alt="MnistExamples" src="https://miro.medium.com/max/700/1*m7-otgfbxiAvSipInHliXw.gif"/>
"""

# Training Parameters
EPOCHS = 2 #10
BATCH_SIZE = 256 #128 

# Loss Function
loss_fn = tf.keras.losses.categorical_crossentropy
# Why don't you try these as well?
#loss_fn = tf.keras.losses.MeanSquaredError()
#loss_fn = tf.keras.losses.Hinge()

# Optimization Method
optimizer = tf.keras.optimizers.Adam(learning_rate=0.001,beta_1=0.9,beta_2=0.999,epsilon=1e-07,amsgrad=False)
# Why don't you try these as well?
# tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.0, nesterov=False)
# tf.keras.optimizers.Adagrad(learning_rate=0.001,initial_accumulator_value=0.1,epsilon=1e-07)

model.compile(loss=loss_fn, optimizer=optimizer, metrics=['accuracy'])
if loss_fn == tf.keras.losses.categorical_crossentropy:
    y_train = tf.keras.utils.to_categorical(y_train0)
    y_test = tf.keras.utils.to_categorical(y_test0)

"""# Training Loop and Testing Results"""

model.fit(x_train, y_train,
          batch_size=BATCH_SIZE,
          epochs=EPOCHS,
          verbose=1) # Verbose controls the output frequency during training



train_loss, train_acc = model.evaluate(x_train, y_train)
test_loss, test_acc = model.evaluate(x_test, y_test)

print('Train accuracy:', train_acc)
print('Test accuracy:', test_acc)

"""## Plot Prediction and Network Output"""

n_images = 3
outs = list(model.predict(x_test[:n_images,:,:,:]))
preds = list(np.argmax(model.predict(x_test[:n_images,:,:,:]), axis=-1))

# Display
for i in range(n_images):
    plt.imshow(x_test[i,:,:,0], cmap='gray')
    plt.show()
    print("Network prediction:", preds[i])
    print("Network Output:", outs[i])

"""# Food for thought

- Artificial Intelligence = Machine Learning = Neural Networks ???
- Moravec’s Paradox: “It is comparatively easy to make computers exhibit adult level performance on intelligence tests, playing checkers or calculating pi to a billion digits, but difficult or impossible to give them the skills of a one-year-old when it comes to perception and mobility… The mental abilities of a child that we take for granted – recognizing a face, lifting a pencil, or walking across a room – in fact solve some of the hardest engineering problems ever conceived… Encoded in the large, highly evolved sensory and motor portions of the human brain is a billion years of experience about the nature of the world and how to survive in it. ”
- AI winters: Over-promising and Under-delivering 
- AI Revolution: Has it happened yet? ([Michael I. Jordan](https://hdsr.mitpress.mit.edu/pub/wot7mkc1/release/9))
"""